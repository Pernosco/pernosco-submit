#!/usr/bin/env python3

import base64
import errno
import glob
import hashlib
import json
import os
import random
import re
import shutil
import subprocess
import sys
import tempfile
import time
import urllib.request
import zipfile

class CustomException(Exception):
    pass

def usage():
    print("Usage:", file=sys.stderr)
    print("%s upload [--title <string>] [--url <url>] [--consent-to-current-privacy-policy] <trace-dir> [<source-path>...]"%sys.argv[0], file=sys.stderr)
    print("  Upload a trace to Pernosco", file=sys.stderr)
    print("    --title <string>: Display the given name in the Pernosco UI and tab title", file=sys.stderr)
    print("    --url <url>: Make the name a link to the given URL", file=sys.stderr)
    print("    --consent-to-current-privacy-policy: Unconditionally consent to the current privacy policy", file=sys.stderr)
    print("  Only source files under the given path(s) will be uploaded.", file=sys.stderr)
    print(file=sys.stderr)
    print("%s keygen <provided-secret>"%sys.argv[0], file=sys.stderr)
    print("  Generate a new public/private key pair for use with Pernosco. Only useful if someone has instructed you to use this.", file=sys.stderr)
    sys.exit(1)

def strip_wrapper(s):
    ret = ""
    for line in s.splitlines():
        if line.startswith("----"):
            continue
        ret += line.strip()
    return ret

def check_executable(executable, package):
    if not shutil.which(executable):
        print("Cannot find `%s`. Please install package `%s`."%(executable, package), file=sys.stderr)
        sys.exit(1)

echo_commands = False
def check_output(process_args, *args, **kwargs):
    if echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.check_output(process_args, *args, **kwargs)

def check_call(process_args, *args, **kwargs):
    if echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.check_call(process_args, *args, **kwargs)

def Popen(process_args, *args, **kwargs):
    if echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.Popen(process_args, *args, **kwargs)

def keygen():
    if len(sys.argv) != 3:
        usage()
    check_executable('openssl', 'openssl')

    provided = sys.argv[2]
    if len(provided.split(',')) != 2:
        print("Invalid <provided-secret> argument", file=sys.stderr)
    full_key = check_output(['openssl', 'ecparam', '-genkey', '-name', 'prime256v1', '-noout'])
    public_key = check_output(['openssl', 'ec', '-pubout'], input=full_key, stderr=subprocess.DEVNULL).decode('utf-8')
    print("Private key: PERNOSCO_USER_SECRET_KEY=%s,%s"%(provided, strip_wrapper(full_key.decode('utf-8'))))
    print("Public key: %s"%strip_wrapper(public_key))

if len(sys.argv) < 2:
    usage()
cmd = sys.argv[1]
if cmd == 'keygen':
    keygen()
    sys.exit(0)
if cmd != 'upload' or len(sys.argv) < 3:
    usage()

check_executable('openssl', 'openssl')
check_executable('tar', 'tar')
check_executable('zstdmt', 'zstd')
if not shutil.which('aws'):
    print("Please install the AWS command-line tools using", file=sys.stderr)
    print("  sudo pip3 install awscli --upgrade", file=sys.stderr)
    print("(Distribution packages may fail due to https://github.com/aws/aws-cli/issues/2403.)", file=sys.stderr)
    sys.exit(1)
if not shutil.which('rr'):
    print("Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
    sys.exit(1)
status = subprocess.run(['rr', 'traceinfo', '/dev/null'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
if status.returncode != 65:
    print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
    sys.exit(1)

consent_to_current_privacy_policy = False
metadata_title = None
metadata_url = None
ignore_warnings = False
while len(sys.argv) > 2:
    if sys.argv[2] == "--consent-to-current-privacy-policy":
        del sys.argv[2]
        consent_to_current_privacy_policy = True
    elif sys.argv[2] == "--title" and len(sys.argv) > 3:
        metadata_title = sys.argv[3]
        if len(metadata_title.encode('utf-8')) > 100:
            print("Title must have max 100 UTF-8 bytes", file=sys.stderr)
            exit(1)
        del sys.argv[2:4]
    elif sys.argv[2] == "--url" and len(sys.argv) > 3:
        metadata_url = sys.argv[3]
        if len(metadata_url.encode('utf-8')) > 1000:
            print("Url must have max 1000 UTF-8 bytes", file=sys.stderr)
            exit(1)
        del sys.argv[2:4]
    elif sys.argv[2] == "--ignore-warnings":
        del sys.argv[2]
        ignore_warnings = True
        print("Ignoring the automated trace check (not recommended!)", file=sys.stderr)
    elif sys.argv[2] == "-x":
        del sys.argv[2]
        echo_commands = True
    else:
        break
if len(sys.argv) < 3:
    usage()
trace_dir = sys.argv[2]
source_dirs = []
for path in sys.argv[3:]:
    if path.startswith('-'):
        print("Unexpected option '%s' in path list; put all options before the trace directory or prepend './'"%path, file=sys.stderr)
        sys.exit(1)
    # Apply realpath since our processing below depends on that
    source_dirs.append(os.path.realpath(path))
if not os.path.isfile("%s/version"%trace_dir):
    print("Can't find rr trace in %s"%trace_dir, file=sys.stderr)
    sys.exit(1)

if not 'PERNOSCO_USER' in os.environ:
    print("PERNOSCO_USER not set", file=sys.stderr)
    sys.exit(1)
pernosco_user = os.environ['PERNOSCO_USER']
if not 'PERNOSCO_GROUP' in os.environ:
    print("PERNOSCO_GROUP not set", file=sys.stderr)
    sys.exit(1)
pernosco_group = os.environ['PERNOSCO_GROUP']
if not 'PERNOSCO_USER_SECRET_KEY' in os.environ:
    print("PERNOSCO_USER_SECRET_KEY not set", file=sys.stderr)
    sys.exit(1)
pernosco_user_secret_key = os.environ['PERNOSCO_USER_SECRET_KEY']

# Fetch current privacy policy version (or None if we fail to get it)
def fetch_privacy_policy_version():
    version_re = re.compile(b'<meta name="policy-version" content="(\d+)">')
    try:
        content = urllib.request.urlopen('https://pernos.co/privacy')
        for line in content.readlines():
            m = version_re.search(line)
            if m:
                return int(m.group(1))
    except:
        pass
    print("Unable to determine privacy policy version!", file=sys.stderr)
    return None

def check_privacy_policy_consent():
    if consent_to_current_privacy_policy:
        return
    policy_version = fetch_privacy_policy_version()
    consent_file_dir = "%s/.local/share/pernosco"%os.environ['HOME']
    consent_file = "%s/consented-to-privacy-policy-version"%consent_file_dir
    try:
        with open(consent_file) as f:
            consented_to_version = int(f.read())
            if consented_to_version == policy_version:
                return
            if policy_version != None:
                print("Privacy policy has changed.", file=sys.stderr)
    except:
        pass
    if not sys.stdin.isatty():
        print("Need to consent to privacy policy, but stdin is not a terminal", file=sys.stderr)
        sys.exit(1)
    if not sys.stdout.isatty():
        print("Need to consent to privacy policy, but stdout is not a terminal", file=sys.stderr)
        sys.exit(1)
    while True:
        s = input("You must consent to the current privacy policy at https://pernos.co/privacy. Do you? (yes/no) ")
        if s == 'no':
            sys.exit(1)
        if s == 'yes':
            break
        print("Please enter 'yes' or 'no'.")
    if policy_version != None:
        os.makedirs(consent_file_dir, exist_ok=True)
        with open(consent_file, "w") as f:
            print(policy_version, file=f)

AVX512_CPUID_EXTENDED_FEATURES_EBX = 0xdc230000
AVX512_CPUID_EXTENDED_FEATURES_ECX = 0x00002c42
AVX512_CPUID_EXTENDED_FEATURES_EDX = 0x0000000c

def has_avx512(cpuid_records):
    for r in cpuid_records:
        if r[0] == 7 and r[1] == 0:
            if ((r[3] & AVX512_CPUID_EXTENDED_FEATURES_EBX) != 0 or
                (r[4] & AVX512_CPUID_EXTENDED_FEATURES_ECX) != 0 or
                (r[5] & AVX512_CPUID_EXTENDED_FEATURES_EDX) != 0):
               return True
    return False

SENSITIVE_ENV_VARS = {
    'PERNOSCO_USER_SECRET_KEY': True,
    'AWS_SECRET_ACCESS_KEY': True,
    'SSHPASS': True,
}

def check_trace(fatal=True):
    output = check_output(['rr', 'traceinfo', trace_dir]).decode('utf-8')
    trace_info = json.loads(output)
    if not 'environ' in trace_info:
        print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)
    if has_avx512(trace_info['cpuidRecords']) and not "PERNOSCO_TEST" in os.environ:
        print("AVX512 enabled when recording trace, but Pernosco does not support AVX512 yet.", file=sys.stderr)
        if fatal:
            print("Re-record with `rr record --disable-cpuid-features-ext 0x%x,0x%x,0x%x`."%(
                AVX512_CPUID_EXTENDED_FEATURES_EBX, AVX512_CPUID_EXTENDED_FEATURES_ECX,
                AVX512_CPUID_EXTENDED_FEATURES_EDX), file=sys.stderr)
            sys.exit(2)
        else:
            print("Ignoring AVX512 enabled. This trace may fail to process!", file=sys.stderr)
    for env in trace_info['environ']:
        name = env.split('=', 1)[0]
        if name in SENSITIVE_ENV_VARS:
            print("Sensitive environment variable %s found in initial recorded process."%name, file=sys.stderr)
            if fatal:
                print("Re-record with environment variable unset.", file=sys.stderr)
                sys.exit(2)
            else:
                print("Ignoring presence of sensitive environment variables. Values will be disclosed!", file=sys.stderr)

def maybe_write_mozilla_metadata():
    for firefox_bin in glob.glob("%s/mmap_*_firefox-bin"%trace_dir):
        original_file_names = check_output(['rr', 'filename', firefox_bin])
        for name in original_file_names.splitlines():
            application_ini = b"%s/application.ini"%os.path.dirname(name)
            if os.path.isfile(application_ini):
                os.makedirs("%s/files.mozilla"%trace_dir, exist_ok=True)
                shutil.copyfile(application_ini, '%s/files.mozilla/application.ini'%trace_dir)
                return

def write_metadata():
    os.makedirs("%s/files.user"%trace_dir, exist_ok=True)
    with open('%s/files.user/user'%trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_user, file=f)
    metadata = {}
    if metadata_title != None:
        metadata['title'] = metadata_title
    if metadata_url != None:
        metadata['url'] = metadata_url
    with open('%s/producer-metadata'%trace_dir, "wt", encoding='utf-8') as f:
        print(json.dumps(metadata), file=f)
    with open('%s/files.user/group'%trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_group, file=f)
    maybe_write_mozilla_metadata()

def rr_pack():
    print("Running 'rr pack'...")
    check_call(['rr', 'pack', trace_dir])

def package_libthread_db():
    for f in ['/usr/lib64/libthread_db.so',
              '/usr/lib/x86_64-linux-gnu/libthread_db.so']:
        if os.path.isfile(f):
            os.makedirs('%s/files.system-debuginfo'%trace_dir, exist_ok=True)
            print("Copying %s into trace..."%f)
            shutil.copyfile(f, '%s/files.system-debuginfo/libthread_db.so'%trace_dir)
            break

# Known Mercurial hosts
mozilla_re = re.compile('https://hg.mozilla.org/(.*)')

def hg_remote_url_to_source_url_generator(remote_url):
    m = mozilla_re.match(remote_url)
    if m:
        if m.group(2) == 'try':
            # Ignore 'try' because it gets purged frequently
            return None
        return lambda rev: ("https://hg.mozilla.org/%s/raw-file/%s/"%(m.group(2), rev), None)
    return None

# Known Git hosts
github_re = re.compile('(https://github.com/|git@github.com:)([^/]+)/(.*)')
gitlab_re = re.compile('(https://gitlab.com/|git@gitlab.com:)([^/]+)/(.*)')
googlesource_re = re.compile('https://([^.]+.googlesource.com)/(.*)')

def strip(s, m):
    if s.endswith(m):
        return s[:(len(s) - len(m))]
    return s

def cinnabar_hg_rev(git_rev, repo_path):
    return check_output(['git', 'cinnabar', 'git2hg', git_rev], cwd=repo_path).decode('utf-8').split()[0]

def git_remote_url_to_source_url_generator(remote_url, repo_path):
    m = github_re.match(remote_url)
    if m:
        return lambda rev: ("https://raw.githubusercontent.com/%s/%s/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = gitlab_re.match(remote_url)
    if m:
        return lambda rev: ("https://gitlab.com/%s/%s/raw/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = googlesource_re.match(remote_url)
    if m:
        # googlesource uses gitiles
        return lambda rev: ("https://%s/%s/+/%s/"%(m.group(1), m.group(2), rev), "?format=TEXT")
    if remote_url.startswith('hg::'):
        # Cinnabar Mercurial host
        hg = hg_remote_url_to_source_url_generator(remote_url[4:])
        if hg:
            return lambda rev: hg(cinnabar_hg_rev(rev, repo_path))
    return None

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def git_remotes(repo_path):
    output = check_output(['git', 'remote', '--verbose'], cwd=repo_path).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, url, token] = line.split()[:3]
        if token != "(fetch)":
            continue
        url_generator = git_remote_url_to_source_url_generator(url, repo_path)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def git_find_rev(repo_path, remotes):
    git = Popen(['git', 'log', '--format=%H %D'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    for line in git.stdout:
        line = line.decode('utf-8')
        revision = line.split()[0]
        for token in line.split():
            if "/" in token:
                remote = token.split('/')[0]
                if remote in remotes:
                    git.kill()
                    git.wait()
                    return (revision, remote)
    git.wait()
    return None

def git_committed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    git = Popen(['git', 'diff', '--name-only', revision, 'HEAD'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

# Computes the files under repo_path that aren't fully committed to HEAD
# (i.e. ignored, untracked, modified in the working area, modified in the git index).
# returns the result as a hash-set.
def git_changed_files(repo_path, files):
    h = {}
    for f in files:
        h[f] = True
    git = Popen(['git', 'status', '--untracked-files=all', '--ignored', '--short'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        line = line.decode('utf-8')
        if line[2] != ' ':
            raise CustomException("Unexpected line: %s"%line)
        file = line[3:].rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

def analyze_git_repo(repo_path, files):
    remotes = git_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = git_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Git repo at %s: Checking for source files changed since revision %s in remote %s (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = git_committed_files(repo_path, revision, files)
    # Collect files changed between HEAD and working dir
    changed_files = git_changed_files(repo_path, files)
    out_files_len = len(out_files)
    out_files.update(changed_files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files with committed changes, %d files changed since HEAD, %d overall"%(out_files_len, len(changed_files), len(out_files)))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def hg_remotes(repo_path):
    output = check_output(['hg', 'paths'], cwd=repo_path).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, equals, url] = line.split()[:3]
        url_generator = hg_remote_url_to_source_url_generator(url)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def hg_find_rev(repo_path, remotes):
    best_rev_num = 0
    best_sha = None
    best_remote = None
    for r in remotes:
        output = check_output(['hg', 'log', '-T', '{rev} {node}', '-r', "ancestor((parents(outgoing('%s') & ancestors(.))) | .)"%r],
                              cwd=repo_path).decode('utf-8')
        [rev_num, sha] = output.split()[:2]
        rev_num = int(rev_num)
        if rev_num > best_rev_num:
            best_rev_num = rev_num
            best_sha = sha
            best_remote = r
    if best_rev_num == 0:
        return None
    return (best_sha, best_remote)

def hg_changed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    hg = Popen(['hg', 'status', '-nmaui', '--rev', revision],
               cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in hg.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    hg.wait()
    return ret

def analyze_hg_repo(repo_path, files):
    remotes = hg_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = hg_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Mercurial repo at %s: Checking for source files changed since revision %s in remote '%s' (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = hg_changed_files(repo_path, revision, files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files changed"%len(out_files))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

def analyze_repo(repo_path, files):
    if os.path.isdir(os.path.join(repo_path, ".git")):
        return analyze_git_repo(repo_path, files)
    if os.path.isdir(os.path.join(repo_path, ".hg")):
        return analyze_hg_repo(repo_path, files)
    return (None, files)

def allowed_file(file):
    for f in source_dirs:
        if file.startswith(f):
            return True
    return False

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

def package_source_files():
    print("Obtaining source file list...")
    output = check_output(['rr', 'sources', trace_dir]).decode('utf-8')
    rr_sources = json.loads(output)

    # Copy external debuginfo into place
    if 'external_debug_info' in rr_sources:
        for e in rr_sources['external_debug_info']:
            build_id = e['build_id']
            dir = "%s/debug/.build-id/%s"%(trace_dir, build_id[:2])
            mkdir_p(dir)
            t = e['type']
            if t == 'debuglink':
                ext = "debug"
            elif t == 'debugaltlink':
                ext = "sup"
            else:
                print("Unknown type '%s' from 'rr sources': is this script out of date? Aborting.", file=sys.stderr)
                sys.exit(1)
            dst = "%s/%s.%s"%(dir, build_id[2:], ext)
            shutil.copy(e['path'], dst)

    out_sources = {};
    out_placeholders = {};
    or_condition = [];
    for b in rr_sources['relevant_binaries']:
        or_condition.append({'binary':b})
    out_sources['condition'] = {'or': or_condition}
    out_placeholders['condition'] = {'or': or_condition}
    explicit_files = []
    out_mounts = []
    out_placeholder_mounts = []
    repo_paths = []
    non_repo_files_count = 0;
    # Mount repos
    for repo_path in rr_sources['files']:
        files = rr_sources['files'][repo_path]
        if repo_path == '':
            non_repo_files_count = len(files)
            explicit_files.extend(files)
            continue
        repo_paths.append(repo_path)
        (repo_mount, modified_files) = analyze_repo(repo_path, files)
        for m in modified_files:
            explicit_files.append(os.path.join(repo_path, m))
        if repo_mount == None:
            continue
        out_mounts.append(repo_mount)
    # Install non-repo files
    print("Packaging %d modified and %d non-repository files..."%(len(explicit_files) - non_repo_files_count, non_repo_files_count))

    with zipfile.ZipFile('%s/files.user/sources.zip'%trace_dir, mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if allowed_file(f):
                zip_file.write(f)
    disallowed_file_count = 0
    with zipfile.ZipFile('%s/files.user/sources-placeholders.zip'%trace_dir, mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if not allowed_file(f):
                content = ("/* This file was not uploaded because the path %s is not under the allowed directories [%s] */"%
                    (f, ", ".join(['"%s"'%d for d in source_dirs])))
                zip_file.writestr(f, content)
                if not ("/.cargo/registry/src/" in f):
                    disallowed_file_count += 1
                    if disallowed_file_count <= 10:
                        print("Not uploading source file %s (add an allowed source directory to the command line?)"%f)
                    if disallowed_file_count == 11:
                        print("(too many disallowed-source-file warnings, suppressing the rest)")
    out_mounts.append({'archive': 'files.user/sources.zip', 'at': '/'})
    out_placeholder_mounts.append({'archive': 'files.user/sources-placeholders.zip', 'at': '/'})
    # Add symlinks
    for s in rr_sources['symlinks']:
        # A symlink at 'from' points to the file at 'to'. So, we want to create
        # a symlink *at* 'from' which is *links* to 'to'.
        out_mounts.append({'link': s['to'], 'at': s['from']})
    # Dump output
    out_sources['files'] = out_mounts
    out_sources['relevance'] = 'Relevant'
    out_placeholders['files'] = out_placeholder_mounts
    out_placeholders['relevance'] = 'NotRelevant'
    out_placeholders['priority'] = -1000
    with open('%s/sources.user'%trace_dir, "wt") as f:
        json.dump([out_sources, out_placeholders], f, indent=2)
    return repo_paths

# The files under these paths are copied into gdbinit/
gdb_paths = [
    # Mozilla
    '.gdbinit',
    '.gdbinit_python',
    'third_party/python/gdbpp',
    # Chromium
    'tools/gdb',
    'third_party/libcxx-pretty-printers',
    'third_party/blink/tools/gdb',
]

def package_gdbinit(repo_paths):
    gdbinit_dir = "%s/gdbinit"%trace_dir
    shutil.rmtree(gdbinit_dir, ignore_errors=True)
    gdbinit_sub_paths = []
    for repo in repo_paths:
        sub_path = repo.replace("/", "_");
        for g in gdb_paths:
            path = os.path.join(repo, g)
            out_path = "%s/%s/%s"%(gdbinit_dir, sub_path, g)
            if os.path.isfile(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying file %s into trace"%path)
                shutil.copy(path, out_path)
            elif os.path.isdir(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying tree %s into trace"%path)
                shutil.copytree(path, out_path, copy_function=shutil.copy)
        # Install our own Pernosco-compatible .gdbinit for Chromium
        if os.path.isfile("%s/%s/tools/gdb/gdb_chrome.py"%(gdbinit_dir, sub_path)):
            with open("%s/%s/.gdbinit"%(gdbinit_dir, sub_path), "wt") as f:
                f.write("""python
import sys
sys.path.insert(0, "/trace/gdbinit/%s/tools/gdb/")
sys.path.insert(0, "/trace/gdbinit/%s/third-party/libcxx-pretty-printers/")
import gdb_chrome
from libcxx.v1.printers import register_libcxx_printers
register_libcxx_printers(None)
gdb.execute('source /trace/gdbinit/%s/tools/gdb/viewg.gdb')
end
"""%(sub_path, sub_path, sub_path))
        if os.path.isfile("%s/%s/.gdbinit"%(gdbinit_dir, sub_path)):
            gdbinit_sub_paths.append(sub_path)
    if len(gdbinit_sub_paths) > 0:
        with open("%s/.gdbinit"%gdbinit_dir, "wt") as f:
            for sub_path in gdbinit_sub_paths:
                print("directory /trace/gdbinit/%s"%sub_path, file=f)
                print("source /trace/gdbinit/%s/.gdbinit"%sub_path, file=f)

def upload():
    [aws_access_key_id, aws_secret_access_key, pernosco_secret_key] = pernosco_user_secret_key.split(',')
    with tempfile.NamedTemporaryFile(mode="w+b") as payload_file:
        print("Compressing to %s..."%payload_file.name)
        with tempfile.TemporaryFile(mode="w+t", encoding='utf-8') as key_file:
            print("-----BEGIN EC PRIVATE KEY-----\n%s\n-----END EC PRIVATE KEY-----"%pernosco_secret_key, file=key_file)
            key_file.seek(0)
            public_key = check_output(['openssl', 'ec', '-pubout'], stdin=key_file, stderr=subprocess.DEVNULL).decode('utf-8')
            p0 = Popen(["tar", "-I", "zstdmt", "--exclude", "db*", "-cf", "-", "."], cwd=trace_dir, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
            p1 = Popen(["tee", payload_file.name], stdin=p0.stdout, stdout=subprocess.PIPE)
            p0.stdout.close()
            os.set_inheritable(key_file.fileno(), True)
            p2 = Popen(["openssl", "dgst", "-sha256", "-sign", "/proc/self/fd/%d"%key_file.fileno()], close_fds=False, stdin=p1.stdout, stdout=subprocess.PIPE)
            p1.stdout.close()
            (sig, err) = p2.communicate()
        if err:
            raise CustomException("openssl failed: %s"%err)
        p0.wait()
        p1.wait()
        if p0.returncode != 0:
            raise CustomException("tar failed: %d"%p0.returncode)
        if p1.returncode != 0:
            raise CustomException("tee failed: %d"%p1.returncode)
        signature = base64.urlsafe_b64encode(sig).decode('utf-8')
        # Create a nonce that's 64 bits of the SHA256 of the signature.
        hasher = hashlib.sha256()
        hasher.update(sig)
        # Strip '=' because it requires %-encoding in URLs
        nonce = base64.urlsafe_b64encode(hasher.digest()[:8]).decode('utf-8').rstrip('=')
        s3_url = "s3://pernosco-upload/%s.tar.zst"%nonce
        print("Uploading %d bytes to %s..."%(os.path.getsize(payload_file.name), s3_url))
        aws_env = dict(os.environ,
            AWS_DEFAULT_REGION='us-east-2',
            AWS_ACCESS_KEY_ID=aws_access_key_id,
            AWS_SECRET_ACCESS_KEY=aws_secret_access_key)
        # Check our credentials before proceeding further.
        checker_args = ["aws", "lambda", "invoke", "--function-name", "upload-credential-check", "--qualifier"]
        if 'PERNOSCO_CREDENTIAL_CHECKER' in os.environ:
            checker_args.extend([os.environ['PERNOSCO_CREDENTIAL_CHECKER']])
        else:
            checker_args.extend(["PROD"])
        checker_args.extend(["--payload", "\"publickey=%s,user=%s,group=%s\""%(strip_wrapper(public_key), pernosco_user, pernosco_group), "/dev/null"])
        p3 = Popen(checker_args, env=aws_env, stdout=subprocess.PIPE)
        check_result = p3.communicate()[0].decode("utf-8")
        if check_result.find("Unhandled") != -1:
            raise CustomException("Your upload credentials are not valid. Check them and try again.")

        # Send the public key with the signature so the server can easily
        # determine which key was used and check that the key is authorized
        metadata = "publickey=%s,signature=%s,user=%s,group=%s"%(strip_wrapper(public_key), signature, pernosco_user, pernosco_group)
        if 'PERNOSCO_EXTRA_METADATA' in os.environ:
            metadata += ",%s"%os.environ['PERNOSCO_EXTRA_METADATA']
        check_call(["aws", "s3", "cp", "--metadata", metadata, payload_file.name, s3_url], env=aws_env)

check_privacy_policy_consent()
check_trace(not ignore_warnings)
write_metadata()
rr_pack()
package_libthread_db()
repo_paths = package_source_files()
package_gdbinit(repo_paths)
upload()
