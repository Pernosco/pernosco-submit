#!/usr/bin/env python3

import argparse
import base64
import errno
import glob
import hashlib
import json
import os
import random
import re
import shutil
import subprocess
import sys
import tempfile
import time
import urllib.parse
import urllib.request
import zipfile

class CustomException(Exception):
    pass

arg_parser = argparse.ArgumentParser()
arg_parser.add_argument("-x", dest='echo_commands', action='store_true', help="Echo spawned command lines")
arg_parser.add_argument("--ignore-warnings", action='store_true', help="Making warnings non-fatal")
arg_subparsers = arg_parser.add_subparsers(dest='subcommand')

keygen_subparser = arg_subparsers.add_parser("keygen", help="Generate a new public/private key pair for use with Pernosco. Only useful if someone has instructed you to use this.")
keygen_subparser.add_argument("provided_secret")

upload_subparser = arg_subparsers.add_parser("upload", help="Upload a trace to Pernosco")
upload_subparser.add_argument("--title", help="Display the given name in the Pernosco UI and tab title")
upload_subparser.add_argument("--url", help="Make the name a link to the given URL")
upload_subparser.add_argument("--consent-to-current-privacy-policy", action='store_true', help="Unconditionally consent to the current privacy policy")
upload_subparser.add_argument("--no-local-sources", action='store_true', help="Don't try to upload any locally modified or generated sources")
upload_subparser.add_argument("--substitute", metavar='LIB_PATH=WITH_PATH', action='append', default=[], help="Find the source for the named library at the named path. Adds WITH_PATH to the allowed source paths.")
upload_subparser.add_argument("--dry-run", metavar="PATH", help="Instead of uploading the package, copies it to the given path. Also writes the command/env to <path>.cmd.")
upload_subparser.add_argument("trace_dir")
upload_subparser.add_argument("source_dirs", nargs="*")

analyze_build_subparser = arg_subparsers.add_parser("analyze-build", help="Create extra-rr-trace-files/ containing source file metadata obtained by scanning built binaries and source file repositories")
analyze_build_subparser.add_argument("--build-dir", help="Specify build directory to use as base for relative DW_AT_comp_dir")
analyze_build_subparser.add_argument("--no-local-sources", action='store_true', help="Don't try to upload any locally modified or generated sources")
analyze_build_subparser.add_argument("--substitute", metavar='LIB_PATH=WITH_PATH', action='append', default=[], help="Find the source for the named library at the named path. Adds WITH_PATH to the allowed source paths.")
analyze_build_subparser.add_argument("--allow-source", metavar='PATH', dest="source_dirs", action='append', default=[], help="Add this path to the list of allowed source paths")
analyze_build_subparser.add_argument("output_dir", help="Where to place extra_rr_trace_files/")
analyze_build_subparser.add_argument("binaries", nargs="*", help="Binary files to scan")

args = arg_parser.parse_args()

random.seed()

def strip_wrapper(s):
    ret = ""
    for line in s.splitlines():
        if line.startswith("----"):
            continue
        ret += line.strip()
    return ret

def check_executable(executable, package):
    if not shutil.which(executable):
        print("Cannot find `%s`. Please install package `%s`."%(executable, package), file=sys.stderr)
        sys.exit(1)

def check_output(process_args, *proc_args, **kwargs):
    if args.echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.check_output(process_args, *proc_args, **kwargs)

def check_call(process_args, *proc_args, **kwargs):
    if args.echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.check_call(process_args, *proc_args, **kwargs)

def Popen(process_args, *proc_args, **kwargs):
    if args.echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.Popen(process_args, *proc_args, **kwargs)

# Fetch current privacy policy version (or None if we fail to get it)
def fetch_privacy_policy_version():
    version_re = re.compile(b'<meta name="policy-version" content="(\d+)">')
    try:
        content = urllib.request.urlopen('https://pernos.co/privacy')
        for line in content.readlines():
            m = version_re.search(line)
            if m:
                return int(m.group(1))
    except:
        pass
    print("Unable to determine privacy policy version!", file=sys.stderr)
    return None

def check_privacy_policy_consent():
    if args.consent_to_current_privacy_policy:
        return
    policy_version = fetch_privacy_policy_version()
    consent_file_dir = "%s/.local/share/pernosco"%os.environ['HOME']
    consent_file = "%s/consented-to-privacy-policy-version"%consent_file_dir
    try:
        with open(consent_file) as f:
            consented_to_version = int(f.read())
            if consented_to_version == policy_version:
                return
            if policy_version != None:
                print("Privacy policy has changed.", file=sys.stderr)
    except:
        pass
    if not sys.stdin.isatty():
        print("Need to consent to privacy policy, but stdin is not a terminal", file=sys.stderr)
        sys.exit(1)
    if not sys.stdout.isatty():
        print("Need to consent to privacy policy, but stdout is not a terminal", file=sys.stderr)
        sys.exit(1)
    while True:
        s = input("You must consent to the current privacy policy at https://pernos.co/privacy. Do you? (yes/no) ")
        if s == 'no':
            sys.exit(1)
        if s == 'yes':
            break
        print("Please enter 'yes' or 'no'.")
    if policy_version != None:
        os.makedirs(consent_file_dir, exist_ok=True)
        with open(consent_file, "w") as f:
            print(policy_version, file=f)

AVX512_CPUID_EXTENDED_FEATURES_EBX = 0xdc230000
AVX512_CPUID_EXTENDED_FEATURES_ECX = 0x00002c42
AVX512_CPUID_EXTENDED_FEATURES_EDX = 0x0000000c

def has_avx512(cpuid_records):
    for r in cpuid_records:
        if r[0] == 7 and r[1] == 0:
            if ((r[3] & AVX512_CPUID_EXTENDED_FEATURES_EBX) != 0 or
                (r[4] & AVX512_CPUID_EXTENDED_FEATURES_ECX) != 0 or
                (r[5] & AVX512_CPUID_EXTENDED_FEATURES_EDX) != 0):
               return True
    return False

SENSITIVE_ENV_VARS = {
    'PERNOSCO_USER_SECRET_KEY': True,
    'AWS_SECRET_ACCESS_KEY': True,
    'DO_API_KEY': True,
    'SSHPASS': True,
}

def check_trace():
    output = check_output(['rr', 'traceinfo', args.trace_dir]).decode('utf-8')
    trace_info = json.loads(output)
    if not 'environ' in trace_info:
        print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)
    if has_avx512(trace_info['cpuidRecords']) and not "PERNOSCO_TEST" in os.environ:
        print("AVX512 enabled when recording trace, but Pernosco does not support AVX512 yet.", file=sys.stderr)
        if not args.ignore_warnings:
            print("Re-record with `rr record --disable-cpuid-features-ext 0x%x,0x%x,0x%x`."%(
                AVX512_CPUID_EXTENDED_FEATURES_EBX, AVX512_CPUID_EXTENDED_FEATURES_ECX,
                AVX512_CPUID_EXTENDED_FEATURES_EDX), file=sys.stderr)
            sys.exit(2)
        else:
            print("Ignoring AVX512 enabled. This trace may fail to process!", file=sys.stderr)
    for env in trace_info['environ']:
        name = env.split('=', 1)[0]
        if name in SENSITIVE_ENV_VARS:
            print("Sensitive environment variable %s found in initial recorded process."%name, file=sys.stderr)
            if not args.ignore_warnings:
                print("Re-record with environment variable unset.", file=sys.stderr)
                sys.exit(2)
            else:
                print("Ignoring presence of sensitive environment variables. Values will be disclosed!", file=sys.stderr)

def maybe_write_mozilla_metadata():
    for firefox_bin in glob.glob("%s/mmap_*_firefox*"%args.trace_dir):
        original_file_names = check_output(['rr', 'filename', firefox_bin])
        for name in original_file_names.splitlines():
            application_ini = b"%s/application.ini"%os.path.dirname(name)
            if os.path.isfile(application_ini):
                os.makedirs("%s/files.mozilla"%args.trace_dir, exist_ok=True)
                shutil.copyfile(application_ini, '%s/files.mozilla/application.ini'%args.trace_dir)
                return

def write_metadata(pernosco_user, pernosco_group):
    os.makedirs("%s/files.user"%args.trace_dir, exist_ok=True)
    with open('%s/files.user/user'%args.trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_user, file=f)
    metadata = {}
    if args.title != None:
        metadata['title'] = args.title
    if args.url != None:
        metadata['url'] = args.url
    with open('%s/producer-metadata'%args.trace_dir, "wt", encoding='utf-8') as f:
        print(json.dumps(metadata), file=f)
    with open('%s/files.user/group'%args.trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_group, file=f)
    maybe_write_mozilla_metadata()

def rr_pack():
    print("Running 'rr pack'...")
    check_call(['rr', 'pack', args.trace_dir])

def package_libthread_db():
    for f in ['/usr/lib64/libthread_db.so',
              '/usr/lib/x86_64-linux-gnu/libthread_db.so']:
        if os.path.isfile(f):
            os.makedirs('%s/files.system-debuginfo'%args.trace_dir, exist_ok=True)
            print("Copying %s into trace..."%f)
            shutil.copyfile(f, '%s/files.system-debuginfo/libthread_db.so'%args.trace_dir)
            break

def package_extra_rr_trace_files():
    extra_file_dirs = dict()
    for binary in glob.glob("%s/mmap_*"%args.trace_dir):
        original_file_names = check_output(['rr', 'filename', binary])
        for name in original_file_names.splitlines():
            extra_files_path = b"%s/extra_rr_trace_files"%os.path.dirname(name)
            if os.path.isdir(extra_files_path):
                extra_file_dirs[extra_files_path] = True
    dir_list = list(extra_file_dirs.keys())
    dir_list.sort()
    for d in dir_list:
        for f in os.listdir(d):
            src_name = os.path.join(d, f)
            dest_name = os.path.join(args.trace_dir.encode('utf-8'), f)
            if os.path.isfile(src_name):
                shutil.copyfile(src_name, dest_name)
            else:
                shutil.copytree(src_name, dest_name)

# Known Mercurial hosts
mozilla_re = re.compile('https://hg.mozilla.org/(.*)')
sourceforge_re = re.compile('http://hg.code.sf.net/(.*)')

def hg_remote_url_to_source_url_generator(remote_url):
    m = mozilla_re.match(remote_url)
    if m:
        if m.group(1) == 'try':
            # Ignore 'try' because it gets purged frequently
            return None
        return lambda rev: ("https://hg.mozilla.org/%s/raw-file/%s/"%(m.group(1), rev), None)
    m = sourceforge_re.match(remote_url)
    if m:
        return lambda rev: ("https://sourceforge.net/%s/ci/%s/tree/"%(m.group(1), rev), "?format=raw")
    return None

# Known Git hosts
github_re = re.compile('(https://github.com/|git@github.com:)([^/]+)/(.*)')
gitlab_re = re.compile('(https://gitlab.com/|git@gitlab.com:)([^/]+)/(.*)')
googlesource_re = re.compile('https://([^.]+.googlesource.com)/(.*)')

def strip(s, m):
    if s.endswith(m):
        return s[:(len(s) - len(m))]
    return s

def cinnabar_hg_rev(git_rev, repo_path):
    return check_output(['git', 'cinnabar', 'git2hg', git_rev], cwd=repo_path).decode('utf-8').split()[0]

def git_remote_url_to_source_url_generator(remote_url, repo_path):
    m = github_re.match(remote_url)
    if m:
        return lambda rev: ("https://raw.githubusercontent.com/%s/%s/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = gitlab_re.match(remote_url)
    if m:
        return lambda rev: ("https://gitlab.com/%s/%s/raw/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = googlesource_re.match(remote_url)
    if m:
        # googlesource uses gitiles
        return lambda rev: ("https://%s/%s/+/%s/"%(m.group(1), m.group(2), rev), "?format=TEXT")
    if remote_url.startswith('hg::'):
        # Cinnabar Mercurial host
        hg = hg_remote_url_to_source_url_generator(remote_url[4:])
        if hg:
            return lambda rev: hg(cinnabar_hg_rev(rev, repo_path))
    return None

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def git_remotes(repo_path):
    output = check_output(['git', 'remote', '--verbose'], cwd=repo_path).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, url, token] = line.split()[:3]
        if token != "(fetch)":
            continue
        url_generator = git_remote_url_to_source_url_generator(url, repo_path)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def git_find_rev(repo_path, remotes):
    git = Popen(['git', 'log', '--format=%H %D'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    for line in git.stdout:
        line = line.decode('utf-8')
        revision = line.split()[0]
        for token in line.split():
            if "/" in token:
                remote = token.split('/')[0]
                if remote in remotes:
                    git.kill()
                    git.wait()
                    return (revision, remote)
    git.wait()
    return None

def git_committed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    git = Popen(['git', 'diff', '--name-only', revision, 'HEAD'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

# Computes the files under repo_path that aren't fully committed to HEAD
# (i.e. ignored, untracked, modified in the working area, modified in the git index).
# returns the result as a hash-set.
def git_changed_files(repo_path, files):
    h = {}
    for f in files:
        h[f] = True
    git = Popen(['git', 'status', '--untracked-files=all', '--ignored', '--short'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        line = line.decode('utf-8')
        if line[2] != ' ':
            raise CustomException("Unexpected line: %s"%line)
        file = line[3:].rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

def analyze_git_repo(repo_path, files):
    remotes = git_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = git_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Git repo at %s: Checking for source files changed since revision %s in remote %s (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = git_committed_files(repo_path, revision, files)
    # Collect files changed between HEAD and working dir
    changed_files = git_changed_files(repo_path, files)
    out_files_len = len(out_files)
    out_files.update(changed_files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files with committed changes, %d files changed since HEAD, %d overall"%(out_files_len, len(changed_files), len(out_files)))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

def safe_env():
    return dict(os.environ, HGPLAIN='1')

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def hg_remotes(repo_path):
    output = check_output(['hg', 'paths'], cwd=repo_path, env=safe_env()).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, equals, url] = line.split()[:3]
        url_generator = hg_remote_url_to_source_url_generator(url)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def hg_find_rev(repo_path, remotes):
    best_rev_num = -1
    best_sha = None
    best_remote = None
    for r in remotes:
        output = check_output(['hg', 'log', '-T', '{rev} {node}', '-r', "ancestor((parents(outgoing('%s') & ancestors(.))) | .)"%r],
                              cwd=repo_path, env=safe_env()).decode('utf-8')
        [rev_num, sha] = output.split()[:2]
        rev_num = int(rev_num)
        if rev_num > best_rev_num:
            best_rev_num = rev_num
            best_sha = sha
            best_remote = r
    if best_rev_num < 0:
        return None
    return (best_sha, best_remote)

def hg_changed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    hg = Popen(['hg', 'status', '-nmaui', '--rev', revision],
               cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=safe_env())
    ret = {}
    for line in hg.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    hg.wait()
    return ret

def analyze_hg_repo(repo_path, files):
    remotes = hg_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = hg_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Mercurial repo at %s: Checking for source files changed since revision %s in remote '%s' (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = hg_changed_files(repo_path, revision, files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files changed"%len(out_files))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

def analyze_repo(repo_path, files):
    if os.path.isdir(os.path.join(repo_path, ".git")):
        return analyze_git_repo(repo_path, files)
    if os.path.isdir(os.path.join(repo_path, ".hg")):
        return analyze_hg_repo(repo_path, files)
    return (None, files)

def allowed_file(source_dirs, file):
    for f in source_dirs:
        if file.startswith(f):
            return True
    return False

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

def package_source_files(source_dirs, comp_dir_substitutions):
    print("Obtaining source file list...")
    try:
        rr_output = check_output(['rr', 'sources'] + comp_dir_substitutions + [args.trace_dir]).decode('utf-8')
    except subprocess.CalledProcessError as x:
        if comp_dir_substitutions:
            print("Error while running rr sources; your installed version of rr may not be new enough to support the --substitute option.")
        else:
            print("Unknown error while running rr sources, aborting")
        sys.exit(1)
    return package_source_files_from_rr_output(source_dirs, rr_output, args.trace_dir, "user", "binary")

def package_source_files_extra(source_dirs, comp_dir_substitutions):
    print("Obtaining source file list...")
    try:
        rr_output = check_output(['rr', 'explicit-sources'] + comp_dir_substitutions + args.binaries).decode('utf-8')
    except subprocess.CalledProcessError as x:
        print("Error while running rr sources; your installed version of rr may not be new enough to support the explicit-sources command.")
        sys.exit(1)
    tag = "extra.%s"%(hex(random.randrange(pow(2,64)))[2:])
    output_dir = "%s/extra_rr_trace_files"%args.output_dir
    mkdir_p("%s/files.%s"%(output_dir, tag))

    return package_source_files_from_rr_output(source_dirs, rr_output, output_dir, tag, "buildid", build_dir=args.build_dir)

def package_source_files_from_rr_output(source_dirs, rr_output, output_dir, tag, condition_type, build_dir=None):
    rr_sources = json.loads(rr_output)

    if 'dwos' in rr_sources:
        dir = "%s/debug/.dwo/"%output_dir
        mkdir_p(dir)
        for e in rr_sources['dwos']:
            path = os.path.join(e['comp_dir'], e['name'])
            dst = "{0:s}/{1:0{2}x}.dwo".format(dir, e['id'], 16)
            shutil.copy(path, dst)

    # Copy external debuginfo into place
    if 'external_debug_info' in rr_sources:
        for e in rr_sources['external_debug_info']:
            build_id = e['build_id']
            dir = "%s/debug/.build-id/%s"%(output_dir, build_id[:2])
            mkdir_p(dir)
            t = e['type']
            if t == 'debuglink':
                ext = "debug"
            elif t == 'debugaltlink':
                ext = "sup"
            else:
                print("Unknown type '%s' from 'rr sources': is this script out of date? Aborting.", file=sys.stderr)
                sys.exit(1)
            dst = "%s/%s.%s"%(dir, build_id[2:], ext)
            shutil.copy(e['path'], dst)

    out_sources = {};
    out_placeholders = {};
    or_condition = [];
    for b in rr_sources['relevant_binaries']:
        or_condition.append({condition_type:b})
    out_sources['condition'] = {'or': or_condition}
    out_placeholders['condition'] = {'or': or_condition}
    explicit_files = []
    out_mounts = []
    out_placeholder_mounts = []
    repo_paths = []
    non_repo_files_count = 0;
    # Mount repos
    for repo_path in rr_sources['files']:
        files = rr_sources['files'][repo_path]
        if repo_path == '':
            non_repo_files_count = len(files)
            explicit_files.extend(files)
            continue
        repo_paths.append(repo_path)
        (repo_mount, modified_files) = analyze_repo(repo_path, files)
        for m in modified_files:
            explicit_files.append(os.path.join(repo_path, m))
        if repo_mount == None:
            continue
        out_mounts.append(repo_mount)
    # Install non-repo files
    print("Packaging %d modified and %d non-repository files..."%(len(explicit_files) - non_repo_files_count, non_repo_files_count))

    with zipfile.ZipFile('%s/files.%s/sources.zip'%(output_dir, tag), mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if allowed_file(source_dirs, f):
                zip_file.write(f)
    disallowed_file_count = 0
    with zipfile.ZipFile('%s/files.%s/sources-placeholders.zip'%(output_dir, tag), mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if not allowed_file(source_dirs, f):
                content = ("/* This file was not uploaded because the path %s is not under the allowed directories [%s] */"%
                    (f, ", ".join(['"%s"'%d for d in source_dirs])))
                zip_file.writestr(f, content)
                if not ("/.cargo/registry/src/" in f):
                    disallowed_file_count += 1
                    if disallowed_file_count <= 10:
                        print("Not uploading source file %s (add an allowed source directory to the command line?)"%f)
                    if disallowed_file_count == 11:
                        print("(too many disallowed-source-file warnings, suppressing the rest)")
    out_mounts.append({'archive': 'files.%s/sources.zip'%tag, 'at': '/'})
    out_placeholder_mounts.append({'archive': 'files.%s/sources-placeholders.zip'%tag, 'at': '/'})
    # Add symlinks
    for s in rr_sources['symlinks']:
        # A symlink at 'from' points to the file at 'to'. So, we want to create
        # a symlink *at* 'from' which is *links* to 'to'.
        out_mounts.append({'link': s['to'], 'at': s['from']})
    # Dump output
    if build_dir != None:
        out_sources['buildDir'] = build_dir
    out_sources['files'] = out_mounts
    out_sources['relevance'] = 'Relevant'
    out_placeholders['files'] = out_placeholder_mounts
    out_placeholders['relevance'] = 'NotRelevant'
    out_placeholders['priority'] = 1000
    with open('%s/sources.%s'%(output_dir, tag), "wt") as f:
        json.dump([out_sources, out_placeholders], f, indent=2)
    return repo_paths

# The files under these paths are copied into gdbinit/
gdb_paths = [
    # Mozilla
    '.gdbinit',
    '.gdbinit_python',
    'third_party/python/gdbpp',
    # Chromium
    'tools/gdb',
    'third_party/libcxx-pretty-printers',
    'third_party/blink/tools/gdb',
]

def package_gdbinit(repo_paths, out_dir):
    shutil.rmtree(out_dir, ignore_errors=True)
    gdbinit_sub_paths = []
    for repo in repo_paths:
        sub_path = repo.replace("/", "_");
        for g in gdb_paths:
            path = os.path.join(repo, g)
            out_path = "%s/%s/%s"%(out_dir, sub_path, g)
            if os.path.isfile(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying file %s into trace"%path)
                shutil.copy(path, out_path)
            elif os.path.isdir(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying tree %s into trace"%path)
                shutil.copytree(path, out_path, copy_function=shutil.copy)
        # Install our own Pernosco-compatible .gdbinit for Chromium
        if os.path.isfile("%s/%s/tools/gdb/gdb_chrome.py"%(out_dir, sub_path)):
            with open("%s/%s/.gdbinit"%(out_dir, sub_path), "wt") as f:
                f.write("""python
import sys
sys.path.insert(0, "/trace/gdbinit/%s/tools/gdb/")
sys.path.insert(0, "/trace/gdbinit/%s/third-party/libcxx-pretty-printers/")
import gdb_chrome
from libcxx.v1.printers import register_libcxx_printers
register_libcxx_printers(None)
gdb.execute('source /trace/gdbinit/%s/tools/gdb/viewg.gdb')
end
"""%(sub_path, sub_path, sub_path))
        if os.path.isfile("%s/%s/.gdbinit"%(out_dir, sub_path)):
            gdbinit_sub_paths.append(sub_path)
    if len(gdbinit_sub_paths) > 0:
        with open("%s/.gdbinit"%out_dir, "wt") as f:
            for sub_path in gdbinit_sub_paths:
                print("directory /trace/gdbinit/%s"%sub_path, file=f)
                print("source /trace/gdbinit/%s/.gdbinit"%sub_path, file=f)

def upload(pernosco_user, pernosco_group, pernosco_user_secret_key):
    [aws_access_key_id, aws_secret_access_key, pernosco_secret_key] = pernosco_user_secret_key.split(',')
    with tempfile.NamedTemporaryFile(mode="w+b") as payload_file:
        print("Compressing to %s..."%payload_file.name)
        with tempfile.TemporaryFile(mode="w+t", encoding='utf-8') as key_file:
            print("-----BEGIN EC PRIVATE KEY-----\n%s\n-----END EC PRIVATE KEY-----"%pernosco_secret_key, file=key_file)
            key_file.seek(0)
            public_key = check_output(['openssl', 'ec', '-pubout'], stdin=key_file, stderr=subprocess.DEVNULL).decode('utf-8')
            p0 = Popen(["tar", "-I", "zstdmt", "--exclude", "./db*", "-cf", "-", "."], cwd=args.trace_dir, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
            p1 = Popen(["tee", payload_file.name], stdin=p0.stdout, stdout=subprocess.PIPE)
            p0.stdout.close()
            os.set_inheritable(key_file.fileno(), True)
            p2 = Popen(["openssl", "dgst", "-sha256", "-sign", "/proc/self/fd/%d"%key_file.fileno()], close_fds=False, stdin=p1.stdout, stdout=subprocess.PIPE)
            p1.stdout.close()
            (sig, err) = p2.communicate()
        if err:
            raise CustomException("openssl failed: %s"%err)
        p0.wait()
        p1.wait()
        if p0.returncode != 0:
            raise CustomException("tar failed: %d"%p0.returncode)
        if p1.returncode != 0:
            raise CustomException("tee failed: %d"%p1.returncode)
        signature = base64.urlsafe_b64encode(sig).decode('utf-8')
        # Create a nonce that's 64 bits of the SHA256 of the signature.
        hasher = hashlib.sha256()
        hasher.update(sig)
        # Strip '=' because it requires %-encoding in URLs
        nonce = base64.urlsafe_b64encode(hasher.digest()[:8]).decode('utf-8').rstrip('=')
        s3_url = "s3://pernosco-upload/%s.tar.zst"%nonce
        really = ''
        if args.dry_run:
              really = " (not really)"
        print("Uploading %d bytes to %s...%s"%(os.path.getsize(payload_file.name), s3_url, really))
        aws_env = dict(os.environ,
            AWS_DEFAULT_REGION='us-east-2',
            AWS_ACCESS_KEY_ID=aws_access_key_id,
            AWS_SECRET_ACCESS_KEY=aws_secret_access_key)

        checker_args = ["aws", "lambda", "invoke", "--function-name", "upload-credential-check", "--qualifier"]
        if 'PERNOSCO_CREDENTIAL_CHECKER' in os.environ:
            checker_args.extend([os.environ['PERNOSCO_CREDENTIAL_CHECKER']])
        else:
            checker_args.extend(["PROD"])
        checker_args.extend(["--payload", "\"publickey=%s,user=%s,group=%s\""%(strip_wrapper(public_key), pernosco_user, pernosco_group), "/dev/null"])

        # Send the public key with the signature so the server can easily
        # determine which key was used and check that the key is authorized
        metadata = "publickey=%s,signature=%s,user=%s,group=%s"%(strip_wrapper(public_key), signature, pernosco_user, pernosco_group)
        if args.title != None:
            metadata += ",title=%s"%urllib.parse.quote(args.title)
        if args.url != None:
            metadata += ",url=%s"%urllib.parse.quote(args.url)

        if 'PERNOSCO_EXTRA_METADATA' in os.environ:
            metadata += ",%s"%os.environ['PERNOSCO_EXTRA_METADATA']
        aws_cmd = ["aws", "s3", "cp", "--metadata", metadata, payload_file.name, s3_url]

        if args.dry_run:
            check_call(["cp", payload_file.name, args.dry_run])
            cmd_obj = dict(checker_cmd=checker_args, aws_cmd=aws_cmd, aws_env=aws_env)
            with open("%s.cmd"%args.dry_run, "w") as cmd:
                print(json.dumps(cmd_obj), file=cmd)
        else:
            # Check our credentials before proceeding further.
            p3 = Popen(checker_args, env=aws_env, stdout=subprocess.PIPE)
            check_result = p3.communicate()[0].decode("utf-8")
            if check_result.find("Unhandled") != -1:
                raise CustomException("Your upload credentials are not valid. Check them and try again.")
            check_call(aws_cmd, env=aws_env)

def keygen_cmd():
    check_executable('openssl', 'openssl')

    if len(args.provided_secret.split(',')) != 2:
        print("Invalid <provided-secret> argument", file=sys.stderr)
    full_key = check_output(['openssl', 'ecparam', '-genkey', '-name', 'prime256v1', '-noout'])
    public_key = check_output(['openssl', 'ec', '-pubout'], input=full_key, stderr=subprocess.DEVNULL).decode('utf-8')
    print("Private key: PERNOSCO_USER_SECRET_KEY=%s,%s"%(args.provided_secret, strip_wrapper(full_key.decode('utf-8'))))
    print("Public key: %s"%strip_wrapper(public_key))

def check_upload_analyze_requirements():
    check_executable('openssl', 'openssl')
    check_executable('tar', 'tar')
    check_executable('zstdmt', 'zstd')
    if not shutil.which('rr'):
        print("Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)
    status = subprocess.run(['rr', 'traceinfo', '/dev/null'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
    if status.returncode != 65:
        print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)

def collect_source_dirs():
    comp_dir_substitutions = []
    source_dirs = []
    for s in args.substitute:
        (library, path) = s.split('=', maxsplit=1)
        if path:
            source_dirs.append(os.path.realpath(path))
        comp_dir_substitutions.extend(["--substitute", s])
    for d in args.source_dirs:
        source_dirs.append(os.path.realpath(d))
    if not args.no_local_sources and len(source_dirs) == 0:
        print("No source directories were provided. Use --no-local-sources if this is intentional.")
        sys.exit(1)
    if len(source_dirs) > 0 and args.no_local_sources:
        print("--no-local-sources was specified but source directories %s were provided"%source_dirs)
        sys.exit(1)
    return (source_dirs, comp_dir_substitutions)

def upload_cmd():
    check_upload_analyze_requirements()
    if not shutil.which('aws'):
        print("Please install the AWS command-line tools using", file=sys.stderr)
        print("  sudo pip3 install awscli --upgrade", file=sys.stderr)
        print("(Distribution packages may fail due to https://github.com/aws/aws-cli/issues/2403.)", file=sys.stderr)
        sys.exit(1)

    if args.ignore_warnings:
        print("Ignoring the automated trace check (not recommended!)", file=sys.stderr)

    if args.title:
        if len(args.title.encode('utf-8')) > 100:
            print("Title must have max 100 UTF-8 bytes", file=sys.stderr)
            exit(1)
    if args.url:
        if len(args.url.encode('utf-8')) > 1000:
            print("Url must have max 1000 UTF-8 bytes", file=sys.stderr)
            exit(1)
    if not os.path.isfile("%s/version"%args.trace_dir):
        print("Can't find rr trace in %s"%args.trace_dir, file=sys.stderr)
        sys.exit(1)

    if not 'PERNOSCO_USER' in os.environ:
        print("PERNOSCO_USER not set", file=sys.stderr)
        sys.exit(1)
    pernosco_user = os.environ['PERNOSCO_USER']
    if not 'PERNOSCO_GROUP' in os.environ:
        print("PERNOSCO_GROUP not set", file=sys.stderr)
        sys.exit(1)
    pernosco_group = os.environ['PERNOSCO_GROUP']
    if not 'PERNOSCO_USER_SECRET_KEY' in os.environ:
        print("PERNOSCO_USER_SECRET_KEY not set", file=sys.stderr)
        sys.exit(1)
    pernosco_user_secret_key = os.environ['PERNOSCO_USER_SECRET_KEY']

    (source_dirs, comp_dir_substitutions) = collect_source_dirs()

    check_privacy_policy_consent()
    check_trace()
    write_metadata(pernosco_user, pernosco_group)
    rr_pack()
    package_libthread_db()
    package_extra_rr_trace_files()
    repo_paths = package_source_files(source_dirs, comp_dir_substitutions)
    package_gdbinit(repo_paths, "%s/gdbinit"%args.trace_dir)
    upload(pernosco_user, pernosco_group, pernosco_user_secret_key)

def analyze_build_cmd():
    check_upload_analyze_requirements()

    (source_dirs, comp_dir_substitutions) = collect_source_dirs()

    repo_paths = package_source_files_extra(source_dirs, comp_dir_substitutions)
    package_gdbinit(repo_paths, "%s/extra_rr_trace_files/gdbinit"%args.output_dir)

if args.subcommand == 'upload':
    upload_cmd()
elif args.subcommand == 'keygen':
    keygen_cmd()
elif args.subcommand == 'analyze-build':
    analyze_build_cmd()
else:
    arg_parser.print_help(sys.stderr)
    sys.exit(1)
