#!/usr/bin/env python3

import argparse
import base64
import errno
import glob
import hashlib
import io
import json
import os
import random
import re
import shutil
import subprocess
import sys
import tempfile
import time
import urllib.parse
import urllib.request
import zipfile

class CustomException(Exception):
    pass

arg_parser = argparse.ArgumentParser()
arg_parser.add_argument("-x", dest='echo_commands', action='store_true', help="Echo spawned command lines")
arg_parser.add_argument("--ignore-warnings", action='store_true', help="Making warnings non-fatal")
arg_subparsers = arg_parser.add_subparsers(dest='subcommand')

keygen_subparser = arg_subparsers.add_parser("keygen", help="Generate a new public/private key pair for use with Pernosco. Only useful if someone has instructed you to use this.")
keygen_subparser.add_argument("provided_secret")

upload_subparser = arg_subparsers.add_parser("upload", help="Upload a trace to Pernosco")
upload_subparser.add_argument("--title", help="Display the given name in the Pernosco UI and tab title")
upload_subparser.add_argument("--url", help="Make the name a link to the given URL")
upload_subparser.add_argument("--consent-to-current-privacy-policy", action='store_true', help="Unconditionally consent to the current privacy policy")
upload_subparser.add_argument("--no-local-sources", action='store_true', help="Don't try to upload any locally modified or generated sources")
upload_subparser.add_argument("--substitute", metavar='LIB_PATH=WITH_PATH', action='append', default=[], help="Find the source for the named library at the named path. Adds WITH_PATH to the allowed source paths.")
upload_subparser.add_argument("--dry-run", metavar="PATH", help="Instead of uploading the package, copies it to the given path. Also writes the command/env to <path>.cmd.")
upload_subparser.add_argument("trace_dir")
upload_subparser.add_argument("source_dirs", nargs="*")

upload_package_subparser = arg_subparsers.add_parser("upload-package", help="Upload a package created using `pernosco-submit upload --dry-run' to Pernosco")
upload_package_subparser.add_argument("--title", help="Display the given name in the Pernosco UI and tab title")
upload_package_subparser.add_argument("--url", help="Make the name a link to the given URL")
upload_package_subparser.add_argument("--consent-to-current-privacy-policy", action='store_true', help="Unconditionally consent to the current privacy policy")
upload_package_subparser.add_argument("package", help="Name of the package file produced by `pernosco-submit upload --dry-run'")

analyze_build_subparser = arg_subparsers.add_parser("analyze-build", help="Create extra-rr-trace-files/ containing source file metadata obtained by scanning built binaries and source file repositories")
analyze_build_subparser.add_argument("--build-dir", help="Specify build directory to use as base for relative DW_AT_comp_dir")
analyze_build_subparser.add_argument("--no-local-sources", action='store_true', help="Don't try to upload any locally modified or generated sources")
analyze_build_subparser.add_argument("--substitute", metavar='LIB_PATH=WITH_PATH', action='append', default=[], help="Find the source for the named library at the named path. Adds WITH_PATH to the allowed source paths.")
analyze_build_subparser.add_argument("--allow-source", metavar='PATH', dest="source_dirs", action='append', default=[], help="Add this path to the list of allowed source paths")
analyze_build_subparser.add_argument("output_dir", help="Where to place extra_rr_trace_files/")
analyze_build_subparser.add_argument("binaries", nargs="*", help="Binary files to scan")

args = arg_parser.parse_args()

random.seed()

def strip_wrapper(s):
    ret = ""
    for line in s.splitlines():
        if line.startswith("----"):
            continue
        ret += line.strip()
    return ret

def check_executable(executable, package):
    if not shutil.which(executable):
        print("Cannot find `%s`. Please install package `%s`."%(executable, package), file=sys.stderr)
        sys.exit(1)

def check_output(process_args, *proc_args, **kwargs):
    if args.echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.check_output(process_args, *proc_args, **kwargs)

def check_call(process_args, *proc_args, **kwargs):
    if args.echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.check_call(process_args, *proc_args, **kwargs)

def Popen(process_args, *proc_args, **kwargs):
    if args.echo_commands:
        print("Running %s"%(" ".join(process_args)))
    return subprocess.Popen(process_args, *proc_args, **kwargs)

# Fetch current privacy policy version (or None if we fail to get it)
def fetch_privacy_policy_version():
    version_re = re.compile(b'<meta name="policy-version" content="(\d+)">')
    try:
        content = urllib.request.urlopen('https://pernos.co/privacy')
        for line in content.readlines():
            m = version_re.search(line)
            if m:
                return int(m.group(1))
    except:
        pass
    print("Unable to determine privacy policy version!", file=sys.stderr)
    return None

def check_privacy_policy_consent():
    if args.consent_to_current_privacy_policy:
        return
    policy_version = fetch_privacy_policy_version()
    consent_file_dir = "%s/.local/share/pernosco"%os.environ['HOME']
    consent_file = "%s/consented-to-privacy-policy-version"%consent_file_dir
    try:
        with open(consent_file) as f:
            consented_to_version = int(f.read())
            if consented_to_version == policy_version:
                return
            if policy_version != None:
                print("Privacy policy has changed.", file=sys.stderr)
    except:
        pass
    if not sys.stdin.isatty():
        print("Need to consent to privacy policy, but stdin is not a terminal", file=sys.stderr)
        sys.exit(1)
    if not sys.stdout.isatty():
        print("Need to consent to privacy policy, but stdout is not a terminal", file=sys.stderr)
        sys.exit(1)
    while True:
        s = input("You must consent to the current privacy policy at https://pernos.co/privacy. Do you? (yes/no) ")
        if s == 'no':
            sys.exit(1)
        if s == 'yes':
            break
        print("Please enter 'yes' or 'no'.")
    if policy_version != None:
        os.makedirs(consent_file_dir, exist_ok=True)
        with open(consent_file, "w") as f:
            print(policy_version, file=f)

AVX512_CPUID_EXTENDED_FEATURES_EBX = 0xdc230000
AVX512_CPUID_EXTENDED_FEATURES_ECX = 0x00002c42
AVX512_CPUID_EXTENDED_FEATURES_EDX = 0x0000000c

def has_avx512(cpuid_records):
    for r in cpuid_records:
        if r[0] == 7 and r[1] == 0:
            if ((r[3] & AVX512_CPUID_EXTENDED_FEATURES_EBX) != 0 or
                (r[4] & AVX512_CPUID_EXTENDED_FEATURES_ECX) != 0 or
                (r[5] & AVX512_CPUID_EXTENDED_FEATURES_EDX) != 0):
               return True
    return False

SENSITIVE_ENV_VARS = {
    'PERNOSCO_USER_SECRET_KEY': True,
    'AWS_SECRET_ACCESS_KEY': True,
    'DO_API_KEY': True,
    'SSHPASS': True,
    'STRIPE_SECRET_KEY': True,
}

def check_trace():
    output = check_output(['rr', 'traceinfo', args.trace_dir]).decode('utf-8')
    trace_info = json.loads(output)
    if not 'environ' in trace_info:
        print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)
    if has_avx512(trace_info['cpuidRecords']) and not "PERNOSCO_TEST" in os.environ:
        print("AVX512 enabled when recording trace, but Pernosco does not support AVX512 yet.", file=sys.stderr)
        if not args.ignore_warnings:
            print("Re-record with `rr record --disable-cpuid-features-ext 0x%x,0x%x,0x%x`."%(
                AVX512_CPUID_EXTENDED_FEATURES_EBX, AVX512_CPUID_EXTENDED_FEATURES_ECX,
                AVX512_CPUID_EXTENDED_FEATURES_EDX), file=sys.stderr)
            sys.exit(2)
        else:
            print("Ignoring AVX512 enabled. This trace may fail to process!", file=sys.stderr)
    for env in trace_info['environ']:
        name = env.split('=', 1)[0]
        if name in SENSITIVE_ENV_VARS:
            print("Sensitive environment variable %s found in initial recorded process."%name, file=sys.stderr)
            if not args.ignore_warnings:
                print("Re-record with environment variable unset.", file=sys.stderr)
                sys.exit(2)
            else:
                print("Ignoring presence of sensitive environment variables. Values will be disclosed!", file=sys.stderr)

# Given a complete source path and a complete destination path,
# replaces the destination with a copy of the source.
def copy_replace_file(src, dst):
    try:
        os.remove(dst)
    except:
        os.makedirs(os.path.dirname(dst), exist_ok=True)
        pass
    shutil.copyfile(src, dst)

def maybe_write_mozilla_metadata():
    for firefox_bin in glob.glob("%s/mmap_*_firefox*"%args.trace_dir):
        original_file_names = check_output(['rr', 'filename', firefox_bin])
        for name in original_file_names.splitlines():
            application_ini = b"%s/application.ini"%os.path.dirname(name)
            if os.path.isfile(application_ini):
                copy_replace_file(application_ini, '%s/files.mozilla/application.ini'%args.trace_dir)
                return

def write_metadata(pernosco_user, pernosco_group):
    os.makedirs("%s/files.user"%args.trace_dir, exist_ok=True)
    with open('%s/files.user/user'%args.trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_user, file=f)
    metadata = {}
    if args.title != None:
        metadata['title'] = args.title
    if args.url != None:
        metadata['url'] = args.url
    with open('%s/producer-metadata'%args.trace_dir, "wt", encoding='utf-8') as f:
        print(json.dumps(metadata), file=f)
    with open('%s/files.user/group'%args.trace_dir, "wt", encoding='utf-8') as f:
        print(pernosco_group, file=f)
    maybe_write_mozilla_metadata()

def rr_pack():
    print("Running 'rr pack'...")
    check_call(['rr', 'pack', args.trace_dir])

def package_libthread_db():
    for f in ['/usr/lib64/libthread_db.so',
              '/usr/lib/x86_64-linux-gnu/libthread_db.so']:
        if os.path.isfile(f):
            print("Copying %s into trace..."%f)
            copy_replace_file(f, '%s/files.system-debuginfo/libthread_db.so'%args.trace_dir)
            break

def package_extra_rr_trace_files():
    extra_file_dirs = dict()
    for binary in glob.glob("%s/mmap_*"%args.trace_dir):
        original_file_names = check_output(['rr', 'filename', binary])
        for name in original_file_names.splitlines():
            extra_files_path = b"%s/extra_rr_trace_files"%os.path.dirname(name)
            if os.path.isdir(extra_files_path):
                extra_file_dirs[extra_files_path] = True
    dir_list = list(extra_file_dirs.keys())
    dir_list.sort()
    for d in dir_list:
        for f in os.listdir(d):
            src_name = os.path.join(d, f)
            dest_name = os.path.join(args.trace_dir.encode('utf-8'), f)
            if os.path.isfile(src_name):
                copy_replace_file(src_name, dest_name)
            else:
                shutil.copytree(src_name, dest_name)

# Known Mercurial hosts
mozilla_re = re.compile('https://hg.mozilla.org/(.*)')
sourceforge_re = re.compile('http://hg.code.sf.net/(.*)')

def hg_remote_url_to_source_url_generator(remote_url):
    m = mozilla_re.match(remote_url)
    if m:
        if m.group(1) == 'try':
            # Ignore 'try' because it gets purged frequently
            return None
        return lambda rev: ("https://hg.mozilla.org/%s/raw-file/%s/"%(m.group(1), rev), None)
    m = sourceforge_re.match(remote_url)
    if m:
        return lambda rev: ("https://sourceforge.net/%s/ci/%s/tree/"%(m.group(1), rev), "?format=raw")
    return None

# Known Git hosts
github_re = re.compile('(https://github.com/|git@github.com:)([^/]+)/(.*)')
gitlab_re = re.compile('(https://gitlab.com/|git@gitlab.com:)([^/]+)/(.*)')
googlesource_re = re.compile('https://([^.]+.googlesource.com)/(.*)')

def strip(s, m):
    if s.endswith(m):
        return s[:(len(s) - len(m))]
    return s

def cinnabar_hg_rev(git_rev, repo_path):
    return check_output(['git', 'cinnabar', 'git2hg', git_rev], cwd=repo_path).decode('utf-8').split()[0]

def git_remote_url_to_source_url_generator(remote_url, repo_path):
    m = github_re.match(remote_url)
    if m:
        return lambda rev: ("https://raw.githubusercontent.com/%s/%s/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = gitlab_re.match(remote_url)
    if m:
        return lambda rev: ("https://gitlab.com/%s/%s/raw/%s/"%(m.group(2), strip(m.group(3), ".git"), rev), None)
    m = googlesource_re.match(remote_url)
    if m:
        # googlesource uses gitiles
        return lambda rev: ("https://%s/%s/+/%s/"%(m.group(1), m.group(2), rev), "?format=TEXT")
    if remote_url.startswith('hg::'):
        # Cinnabar Mercurial host
        hg = hg_remote_url_to_source_url_generator(remote_url[4:])
        if hg:
            return lambda rev: hg(cinnabar_hg_rev(rev, repo_path))
    return None

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def git_remotes(repo_path):
    output = check_output(['git', 'remote', '--verbose'], cwd=repo_path).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, url, token] = line.split()[:3]
        if token != "(fetch)":
            continue
        url_generator = git_remote_url_to_source_url_generator(url, repo_path)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def git_find_rev(repo_path, remotes):
    git = Popen(['git', 'log', '--format=%H %D'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    for line in git.stdout:
        line = line.decode('utf-8')
        revision = line.split()[0]
        for token in line.split():
            if "/" in token:
                remote = token.split('/')[0]
                if remote in remotes:
                    git.kill()
                    git.wait()
                    return (revision, remote)
    git.wait()
    return None

def git_committed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    git = Popen(['git', 'diff', '--name-only', revision, 'HEAD'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

# Computes the files under repo_path that aren't fully committed to HEAD
# (i.e. ignored, untracked, modified in the working area, modified in the git index).
# returns the result as a hash-set.
def git_changed_files(repo_path, files):
    h = {}
    for f in files:
        h[f] = True
    git = Popen(['git', 'status', '--untracked-files=all', '--ignored', '--short'],
                cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret = {}
    for line in git.stdout:
        line = line.decode('utf-8')
        if line[2] != ' ':
            raise CustomException("Unexpected line: %s"%line)
        file = line[3:].rstrip()
        if file in h:
            ret[file] = True
    git.wait()
    return ret

def analyze_git_repo(repo_path, files):
    remotes = git_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = git_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Git repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Git repo at %s: Checking for source files changed since revision %s in remote %s (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = git_committed_files(repo_path, revision, files)
    # Collect files changed between HEAD and working dir
    changed_files = git_changed_files(repo_path, files)
    out_files_len = len(out_files)
    out_files.update(changed_files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files with committed changes, %d files changed since HEAD, %d overall"%(out_files_len, len(changed_files), len(out_files)))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

def safe_env():
    return dict(os.environ, HGPLAIN='1')

# Returns a hash of remote names to generators of URLs Pernosco can use to fetch files
def hg_remotes(repo_path):
    output = check_output(['hg', 'paths'], cwd=repo_path, env=safe_env()).decode('utf-8')
    ret = {}
    for line in output.splitlines():
        [remote, equals, url] = line.split()[:3]
        url_generator = hg_remote_url_to_source_url_generator(url)
        if url_generator != None:
            ret[remote] = url_generator
    return ret

def hg_find_rev(repo_path, remotes):
    best_rev_num = -1
    best_sha = None
    best_remote = None
    for r in remotes:
        output = check_output(['hg', 'log', '-T', '{rev} {node}', '-r', "ancestor((parents(outgoing('%s') & ancestors(.))) | .)"%r],
                              cwd=repo_path, env=safe_env()).decode('utf-8')
        [rev_num, sha] = output.split()[:2]
        rev_num = int(rev_num)
        if rev_num > best_rev_num:
            best_rev_num = rev_num
            best_sha = sha
            best_remote = r
    if best_rev_num < 0:
        return None
    return (best_sha, best_remote)

def hg_changed_files(repo_path, revision, files):
    h = {}
    for f in files:
        h[f] = True
    hg = Popen(['hg', 'status', '-nmaui', '--rev', revision],
               cwd=repo_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, env=safe_env())
    ret = {}
    for line in hg.stdout:
        file = line.decode('utf-8').rstrip()
        if file in h:
            ret[file] = True
    hg.wait()
    return ret

def analyze_hg_repo(repo_path, files):
    remotes = hg_remotes(repo_path)
    if len(remotes) == 0:
        print("No remotes found for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    r = hg_find_rev(repo_path, remotes)
    if not r:
        print("Can't find usable remote master for Mercurial repo %s, packaging files instead..."%repo_path)
        return (None, files)
    (revision, remote) = r
    (url, url_suffix) = remotes[remote](revision)
    print("Mercurial repo at %s: Checking for source files changed since revision %s in remote '%s' (access via %s)..."%(repo_path, revision, remote, url), end="")
    # Collect files changed between `revision` and HEAD
    out_files = hg_changed_files(repo_path, revision, files)
    if len(out_files) == 0:
        print(" no changes")
    else:
        print(" %d files changed"%len(out_files))
    mount = {'url': url, 'at': repo_path}
    if url_suffix:
         mount['urlSuffix'] = url_suffix
    return (mount, out_files)

def analyze_repo(repo_path, files):
    if os.path.isdir(os.path.join(repo_path, ".git")):
        return analyze_git_repo(repo_path, files)
    if os.path.isdir(os.path.join(repo_path, ".hg")):
        return analyze_hg_repo(repo_path, files)
    return (None, files)

def allowed_file(source_dirs, file):
    for f in source_dirs:
        if file.startswith(f):
            return True
    return False

def run_rr_sources(comp_dir_substitutions, cmd, params):
    print("Obtaining source file list...")
    rr_comp_dir_substitutions = []
    for v in comp_dir_substitutions:
        rr_comp_dir_substitutions.append("--substitute")
        rr_comp_dir_substitutions.append("%s=%s"%(v[0], v[1]))
    try:
        rr_output = check_output(['rr', cmd] + rr_comp_dir_substitutions + params).decode('utf-8')
    except subprocess.CalledProcessError as x:
        if comp_dir_substitutions:
            print("Error while running rr sources; your installed version of rr may not be new enough to support the --substitute option.")
        else:
            print("Unknown error while running rr sources, aborting")
        sys.exit(1)
    return rr_output

def package_source_files(source_dirs, comp_dir_substitutions):
    rr_output = run_rr_sources(comp_dir_substitutions, 'sources', [args.trace_dir])
    return package_source_files_from_rr_output(source_dirs, rr_output, comp_dir_substitutions, args.trace_dir, "user", "binary")

def package_source_files_extra(source_dirs, comp_dir_substitutions):
    rr_output = run_rr_sources(comp_dir_substitutions, 'explicit-sources', args.binaries)
    tag = "extra.%s"%(hex(random.randrange(pow(2,64)))[2:])
    output_dir = "%s/extra_rr_trace_files"%args.output_dir
    os.makedirs("%s/files.%s"%(output_dir, tag), exist_ok=True)
    return package_source_files_from_rr_output(source_dirs, rr_output, comp_dir_substitutions, output_dir, tag, "buildid", build_dir=args.build_dir)

def package_source_files_from_rr_output(source_dirs, rr_output, comp_dir_substitutions, output_dir, tag, condition_type, build_dir=None):
    rr_sources = json.loads(rr_output)

    if 'dwos' in rr_sources:
        dir = "%s/debug/.dwo/"%output_dir
        for e in rr_sources['dwos']:
            path = os.path.join(e['comp_dir'], e['name'])
            dst = "{0:s}/{1:0{2}x}.dwo".format(dir, e['id'], 16)
            copy_replace_file(path, dst)

    # Copy external debuginfo into place
    if 'external_debug_info' in rr_sources:
        for e in rr_sources['external_debug_info']:
            build_id = e['build_id']
            dir = "%s/debug/.build-id/%s"%(output_dir, build_id[:2])
            t = e['type']
            if t == 'debuglink':
                ext = "debug"
            elif t == 'debugaltlink':
                ext = "sup"
            else:
                print("Unknown type '%s' from 'rr sources': is this script out of date? Aborting.", file=sys.stderr)
                sys.exit(1)
            dst = "%s/%s.%s"%(dir, build_id[2:], ext)
            copy_replace_file(e['path'], dst)

    out_sources = {};
    out_placeholders = {};
    or_condition = [];
    for b in rr_sources['relevant_binaries']:
        or_condition.append({condition_type:b})
    out_sources['condition'] = {'or': or_condition}
    out_placeholders['condition'] = {'or': or_condition}
    explicit_files = []
    out_mounts = []
    out_placeholder_mounts = []
    repo_paths = []
    non_repo_files_count = 0;
    # Mount repos
    for repo_path in rr_sources['files']:
        files = rr_sources['files'][repo_path]
        if repo_path == '':
            non_repo_files_count = len(files)
            explicit_files.extend(files)
            continue
        repo_paths.append(repo_path)
        (repo_mount, modified_files) = analyze_repo(repo_path, files)
        for m in modified_files:
            explicit_files.append(os.path.join(repo_path, m))
        if repo_mount == None:
            continue
        out_mounts.append(repo_mount)
    # Install non-repo files
    print("Packaging %d modified and %d non-repository files..."%(len(explicit_files) - non_repo_files_count, non_repo_files_count))

    with zipfile.ZipFile('%s/files.%s/sources.zip'%(output_dir, tag), mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if allowed_file(source_dirs, f):
                zip_file.write(f)
    disallowed_file_count = 0
    with zipfile.ZipFile('%s/files.%s/sources-placeholders.zip'%(output_dir, tag), mode='w', compression=zipfile.ZIP_DEFLATED) as zip_file:
        for f in explicit_files:
            if not allowed_file(source_dirs, f):
                content = ("/* This file was not uploaded because the path %s is not under the allowed directories [%s] */"%
                    (f, ", ".join(['"%s"'%d for d in source_dirs])))
                zip_file.writestr(f, content)
                if not ("/.cargo/registry/src/" in f):
                    disallowed_file_count += 1
                    if disallowed_file_count <= 10:
                        print("Not uploading source file %s (add an allowed source directory to the command line?)"%f)
                    if disallowed_file_count == 11:
                        print("(too many disallowed-source-file warnings, suppressing the rest)")
    out_mounts.append({'archive': 'files.%s/sources.zip'%tag, 'at': '/'})
    out_placeholder_mounts.append({'archive': 'files.%s/sources-placeholders.zip'%tag, 'at': '/'})
    # Add symlinks
    for s in rr_sources['symlinks']:
        # A symlink at 'from' points to the file at 'to'. So, we want to create
        # a symlink *at* 'from' which is *links* to 'to'.
        out_mounts.append({'link': s['to'], 'at': s['from']})
    # Dump output
    if build_dir != None:
        out_sources['buildDir'] = build_dir
    out_sources['files'] = out_mounts
    out_sources['relevance'] = 'Relevant'
    out_placeholders['files'] = out_placeholder_mounts
    out_placeholders['relevance'] = 'NotRelevant'
    out_placeholders['priority'] = 1000

    all_rules = [out_sources, out_placeholders]
    for v in comp_dir_substitutions:
        out_substitution = {}
        out_substitution['condition'] = {'binary': v[0]};
        out_substitution['overrideCompDir'] = v[1];
        all_rules.append(out_substitution)

    with open('%s/sources.%s'%(output_dir, tag), "wt") as f:
        json.dump(all_rules, f, indent=2)
    return repo_paths

# The files under these paths are copied into gdbinit/
gdb_paths = [
    # Mozilla
    '.gdbinit',
    '.gdbinit_python',
    'third_party/python/gdbpp',
    # Chromium
    'tools/gdb',
    'third_party/libcxx-pretty-printers',
    'third_party/blink/tools/gdb',
]

def package_gdbinit(repo_paths, out_dir):
    shutil.rmtree(out_dir, ignore_errors=True)
    gdbinit_sub_paths = []
    for repo in repo_paths:
        sub_path = repo.replace("/", "_");
        for g in gdb_paths:
            path = os.path.join(repo, g)
            out_path = "%s/%s/%s"%(out_dir, sub_path, g)
            if os.path.isfile(path):
                print("Copying file %s into trace"%path)
                copy_replace_file(path, out_path)
            elif os.path.isdir(path):
                os.makedirs(os.path.dirname(out_path), exist_ok=True)
                print("Copying tree %s into trace"%path)
                shutil.copytree(path, out_path, copy_function=shutil.copyfile)
        # Install our own Pernosco-compatible .gdbinit for Chromium
        if os.path.isfile("%s/%s/tools/gdb/gdb_chrome.py"%(out_dir, sub_path)):
            with open("%s/%s/.gdbinit"%(out_dir, sub_path), "wt") as f:
                f.write("""python
import sys
sys.path.insert(0, "/trace/gdbinit/%s/tools/gdb/")
sys.path.insert(0, "/trace/gdbinit/%s/third-party/libcxx-pretty-printers/")
import gdb_chrome
from libcxx.v1.printers import register_libcxx_printers
register_libcxx_printers(None)
gdb.execute('source /trace/gdbinit/%s/tools/gdb/viewg.gdb')
end
"""%(sub_path, sub_path, sub_path))
        if os.path.isfile("%s/%s/.gdbinit"%(out_dir, sub_path)):
            gdbinit_sub_paths.append(sub_path)
    if len(gdbinit_sub_paths) > 0:
        with open("%s/.gdbinit"%out_dir, "wt") as f:
            for sub_path in gdbinit_sub_paths:
                print("directory /trace/gdbinit/%s"%sub_path, file=f)
                print("source /trace/gdbinit/%s/.gdbinit"%sub_path, file=f)

def prepare_upload_data(payload_file, pernosco_secret_key):
    """
    Create a package for uploading and return cryptographic data required to
    upload said package.

        Parameters:
            pernosco_secret_key (str): Pernosco secret key
            payload_file (file object): Package file to be created

        Return:
            A tuple of cryptographic data required for uploading newly created package: (public_key, signature, nonce)
    """
    with tempfile.TemporaryFile(mode="w+t", encoding='utf-8') as key_file:
        print("-----BEGIN EC PRIVATE KEY-----\n%s\n-----END EC PRIVATE KEY-----"%pernosco_secret_key, file=key_file)
        key_file.seek(0)
        public_key = check_output(['openssl', 'ec', '-pubout'], stdin=key_file, stderr=subprocess.DEVNULL).decode('utf-8')
        p0 = Popen(["tar", "-I", "zstdmt", "--exclude", "./db*", "-cf", "-", "."], cwd=args.trace_dir, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
        p1 = Popen(["tee", payload_file.name], stdin=p0.stdout, stdout=subprocess.PIPE)
        p0.stdout.close()
        os.set_inheritable(key_file.fileno(), True)
        p2 = Popen(["openssl", "dgst", "-sha256", "-sign", "/proc/self/fd/%d"%key_file.fileno()], close_fds=False, stdin=p1.stdout, stdout=subprocess.PIPE)
        p1.stdout.close()
        (sig, err) = p2.communicate()
    if err:
        raise CustomException("openssl failed: %s"%err)
    p0.wait()
    p1.wait()
    if p0.returncode != 0:
        raise CustomException("tar failed: %d"%p0.returncode)
    if p1.returncode != 0:
        raise CustomException("tee failed: %d"%p1.returncode)
    signature = base64.urlsafe_b64encode(sig).decode('utf-8')
    # Create a nonce that's 64 bits of the SHA256 of the signature.
    hasher = hashlib.sha256()
    hasher.update(sig)
    # Strip '=' because it requires %-encoding in URLs
    nonce = base64.urlsafe_b64encode(hasher.digest()[:8]).decode('utf-8').rstrip('=')
    return (public_key, signature, nonce)

def get_aws_environment(aws_access_key_id, aws_secret_access_key):
    """
    Return a dictionary with current environment variables plus AWS access
    credentials.

        Parameters:
            aws_access_key_id (str): AWS access key ID
            aws_secret_access_key (str): AWS secret access key

        Returns:
            Dictionary with current environment variables plus AWS access
            credentials.
    """
    aws_env = dict(os.environ,
        AWS_DEFAULT_REGION='us-east-2',
        AWS_ACCESS_KEY_ID=aws_access_key_id,
        AWS_SECRET_ACCESS_KEY=aws_secret_access_key)
    return aws_env

def get_metadata_for_upload(public_key, signature, pernosco_user, pernosco_group):
    """
    Return metadata required for uploading a package to Pernosco.

        Parameters:
            public_key (str): Public key for Pernosco user
            signature (str): Message digest for the package to be uploaded
            pernosco_user (str): Pernosco user
            pernosco_group (str): Pernosco group

        Returns:
            metadata (str): A string of comma-separated metadata values
    """
    # Send the public key with the signature so the server can easily
    # determine which key was used and check that the key is authorized
    metadata = "publickey=%s,signature=%s,user=%s,group=%s"%(strip_wrapper(public_key), signature, pernosco_user, pernosco_group)
    if args.title != None:
        metadata += ",title=%s"%urllib.parse.quote(args.title)
    if args.url != None:
        metadata += ",url=%s"%urllib.parse.quote(args.url)

    if 'PERNOSCO_EXTRA_METADATA' in os.environ:
        metadata += ",%s"%os.environ['PERNOSCO_EXTRA_METADATA']
    return metadata

def append_crypto_data_to_package(package_file_name, crypto_data):
    """
    Append cryptographic data required to upload the package at a later time
    to the package.

        Parameters:
            package_file_name (str): Name of the package file to be uploaded later
            crypto_data (dict): Cryptographic data required to upload the package
    """
    print("Appending cryptographic data required for upload to %s"%package_file_name)
    crypto_data_json = json.dumps(crypto_data)
    with open(package_file_name, "a") as package_file:
        print(crypto_data_json, file=package_file)
        print("%d"%len(crypto_data_json), end="", file=package_file)

def upload_file(nonce, signature, aws_access_key_id, aws_secret_access_key, public_key, pernosco_user, pernosco_group, metadata, payload_file_name):
    """
    Upload the specified payload file to Pernosco.

        Parameters:
            nonce (str): Nonce for uploaded file name
            signature (str): Message digest for the package to be uploaded
            aws_access_key_id (str): AWS access key ID
            aws_secret_access_key (str): AWS secret access key
            public_key (str): Public key for Pernosco user
            pernosco_user (str): Pernosco user
            pernosco_group (str): Pernosco group
            metadata (str): Metadata required for the uplaod
            payload_file_name (str): File to be uploaded
    """
    s3_url = "s3://pernosco-upload/%s.tar.zst"%nonce
    really = ''
    if args.subcommand == 'upload' and args.dry_run:
        really = " (not really)"
    print("Uploading %d bytes to %s...%s"%(os.path.getsize(payload_file_name), s3_url, really))

    aws_env = get_aws_environment(aws_access_key_id, aws_secret_access_key)

    checker_args = ["aws", "lambda", "invoke", "--function-name", "upload-credential-check", "--qualifier"]
    if 'PERNOSCO_CREDENTIAL_CHECKER' in os.environ:
        checker_args.extend([os.environ['PERNOSCO_CREDENTIAL_CHECKER']])
    else:
        checker_args.extend(["PROD"])
    checker_args.extend(["--payload", "\"publickey=%s,user=%s,group=%s\""%(strip_wrapper(public_key), pernosco_user, pernosco_group), "/dev/null"])

    aws_cmd = ["aws", "s3", "cp", "--metadata", metadata, payload_file_name, s3_url]

    if args.subcommand == 'upload' and args.dry_run:
        check_call(["cp", payload_file_name, args.dry_run])
        cmd_obj = dict(checker_cmd=checker_args, aws_cmd=aws_cmd, aws_env=aws_env)
        with open("%s.cmd"%args.dry_run, "w") as cmd:
            print(json.dumps(cmd_obj), file=cmd)

        crypto_data = {
            'public_key': public_key.strip(),
            'signature': signature.strip(),
            'nonce': nonce.strip()
        }
        append_crypto_data_to_package(args.dry_run, crypto_data)
    else:
        # Check our credentials before proceeding further.
        p3 = Popen(checker_args, env=aws_env, stdout=subprocess.PIPE)
        check_result = p3.communicate()[0].decode("utf-8")
        if check_result.find("Unhandled") != -1:
            raise CustomException("Your upload credentials are not valid. Check them and try again.")
        check_call(aws_cmd, env=aws_env)

def upload(pernosco_user, pernosco_group, pernosco_user_secret_key):
    """
    Upload a trace to Pernosco.

        Parameters:
            pernosco_user (str): Pernosco user
            pernosco_group (str): Pernosco group
            pernosco_user_secret_key (str): Secret credentials required for Pernosco
    """
    with tempfile.NamedTemporaryFile(mode="w+b") as payload_file:
        print("Compressing to %s..."%payload_file.name)
        [aws_access_key_id, aws_secret_access_key, pernosco_secret_key] = pernosco_user_secret_key.split(',')
        (public_key, signature, nonce) = prepare_upload_data(payload_file, pernosco_secret_key)
        metadata = get_metadata_for_upload(public_key, signature, pernosco_user, pernosco_group)
        upload_file(nonce, signature, aws_access_key_id, aws_secret_access_key, public_key, pernosco_user, pernosco_group, metadata, payload_file.name)

def extract_crypto_data_from_package(package_file_name):
    """
    Extract the persisted cryptographic data required to upload the specified package file
    from said package file.
    NOTE: This alters the package file by removing the extracted cryptographic data.

        Parameters:
            package_file_name (str): Name of the package file to be uploaded to Pernosco

        Returns:
            crypto_data (dict): Cryptographic data required for uploading specified package
    """
    print("Extracting cryptographic data required for upload from %s"%package_file_name)

    # First, get the length of the cryptographic data JSON string from the end
    # of the file.
    crypto_file = open(package_file_name, "r+")
    crypto_file.seek(0, io.SEEK_END)

    is_reading_json_len = True
    offset = crypto_file.tell() - 1
    json_len_str = ""
    while is_reading_json_len:
        crypto_file.seek(offset, io.SEEK_SET)
        byte = crypto_file.read(1)
        if byte == "\n":
            is_reading_json_len = False
        else:
            json_len_str += byte
            offset -= 1

    # Reverse the length string since it was read backwards from the end of the fle.
    json_len_str= json_len_str[::-1]
    json_len = int(json_len_str)

    # `offset` is currently at the '\n' following the crytpographic data JSON string.
    # Update it so that it's at the beginning of the JSON string.
    offset -= json_len

    # Second, read the cryptographic data JSON string.
    crypto_file.seek(offset, io.SEEK_SET)
    crypto_data_str = crypto_file.readline()
    crypto_data = json.loads(crypto_data_str)

    # Third, truncate the file to restore the original package file without any
    # cryptographic data appended to the end.

    # Since `offset` is the byte index of the cryptographic data's beginning,
    # it is also the size of the original package file without the
    # cryptographic data.
    original_package_len = offset
    crypto_file.truncate(original_package_len)
    crypto_file.close()

    return crypto_data

def upload_package(pernosco_user, pernosco_group, pernosco_user_secret_key, package_file_name):
    """
    Upload the specified package file to Pernosco.

        Parameters:
            pernosco_user (str): Pernosco user
            pernosco_group (str): Pernosco group
            pernosco_user_secret_key (str): Secret credentials required for Pernosco
            package_file_name (str): Name of the package file to be uploaded to Pernosco
    """
    # Extract cryptographic data required to upload the package from the
    # package file; remove said cryptographic data from the package in order to
    # make the package valid for an upload.
    crypto_data = extract_crypto_data_from_package(package_file_name)
    (public_key, signature, nonce) = crypto_data["public_key"], crypto_data["signature"], crypto_data["nonce"]

    metadata = get_metadata_for_upload(public_key, signature, pernosco_user, pernosco_group)
    [aws_access_key_id, aws_secret_access_key, pernosco_secret_key] = pernosco_user_secret_key.split(',')

    try:
        upload_file(nonce, signature, aws_access_key_id, aws_secret_access_key, public_key, pernosco_user, pernosco_group, metadata, package_file_name)
    finally:
        # Add the cryptographic data required to upload the package back into the
        # package file.
        append_crypto_data_to_package(package_file_name, crypto_data)

def keygen_cmd():
    check_executable('openssl', 'openssl')

    if len(args.provided_secret.split(',')) != 2:
        print("Invalid <provided-secret> argument", file=sys.stderr)
    full_key = check_output(['openssl', 'ecparam', '-genkey', '-name', 'prime256v1', '-noout'])
    public_key = check_output(['openssl', 'ec', '-pubout'], input=full_key, stderr=subprocess.DEVNULL).decode('utf-8')
    print("Private key: PERNOSCO_USER_SECRET_KEY=%s,%s"%(args.provided_secret, strip_wrapper(full_key.decode('utf-8'))))
    print("Public key: %s"%strip_wrapper(public_key))

def check_upload_analyze_requirements():
    check_executable('openssl', 'openssl')
    check_executable('tar', 'tar')
    check_executable('zstdmt', 'zstd')
    if not shutil.which('rr'):
        print("Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)
    status = subprocess.run(['rr', 'traceinfo', '/dev/null'], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)
    if status.returncode != 65:
        print("rr is out of date. Please install `rr` master and make sure it's on your $PATH.", file=sys.stderr)
        sys.exit(1)

def collect_source_dirs():
    source_dirs = []
    comp_dir_substitutions = []
    for s in args.substitute:
        (library, path) = s.split('=', maxsplit=1)
        if not path:
            print("Missing path in %s"%s, file=sys.stderr)
            sys.exit(1)
        source_dirs.append(os.path.realpath(path))
        comp_dir_substitutions.append([library, path])
    for d in args.source_dirs:
        source_dirs.append(os.path.realpath(d))
    if not args.no_local_sources and len(source_dirs) == 0:
        print("No source directories were provided. Use --no-local-sources if this is intentional.", file=sys.stderr)
        sys.exit(1)
    if len(source_dirs) > 0 and args.no_local_sources:
        print("--no-local-sources was specified but source directories %s were provided"%source_dirs, file=sys.stderr)
        sys.exit(1)
    return (source_dirs, comp_dir_substitutions)

def get_config_var(name):
    env_var = "PERNOSCO_%s"%(name.upper())
    if env_var in os.environ:
        return os.environ[env_var]
    if 'HOME' in os.environ:
        path = "%s/.config/pernosco/%s"%(os.environ['HOME'], name)
        try:
            with open(path, "r") as file:
                return file.read().replace('\n', '')
        except:
            pass
    print("Can't find %s or ~/.config/pernosco/%s"%(env_var, name), file=sys.stderr)
    sys.exit(1)

def check_for_aws_command():
    """
    Check that the `aws' command-line tools are installed.
    """
    if not shutil.which('aws'):
        print("Please install the AWS command-line tools using", file=sys.stderr)
        print("  sudo pip3 install awscli --upgrade", file=sys.stderr)
        print("(Distribution packages may fail due to https://github.com/aws/aws-cli/issues/2403.)", file=sys.stderr)
        sys.exit(1)

def check_title_and_url_args():
    """
    Check that --title and --url command-line arguments are valid.
    """
    if args.title:
        if len(args.title.encode('utf-8')) > 100:
            print("Title must have max 100 UTF-8 bytes", file=sys.stderr)
            exit(1)
    if args.url:
        if len(args.url.encode('utf-8')) > 1000:
            print("Url must have max 1000 UTF-8 bytes", file=sys.stderr)
            exit(1)

def upload_cmd():
    check_upload_analyze_requirements()
    check_for_aws_command()

    if args.ignore_warnings:
        print("Ignoring the automated trace check (not recommended!)", file=sys.stderr)

    check_title_and_url_args()

    if not os.path.isfile("%s/version"%args.trace_dir):
        print("Can't find rr trace in %s"%args.trace_dir, file=sys.stderr)
        sys.exit(1)

    pernosco_user = get_config_var('user')
    pernosco_group = get_config_var('group')
    pernosco_user_secret_key = get_config_var('user_secret_key')

    (source_dirs, comp_dir_substitutions) = collect_source_dirs()

    check_privacy_policy_consent()
    check_trace()
    write_metadata(pernosco_user, pernosco_group)
    rr_pack()
    package_libthread_db()
    package_extra_rr_trace_files()
    repo_paths = package_source_files(source_dirs, comp_dir_substitutions)
    package_gdbinit(repo_paths, "%s/gdbinit"%args.trace_dir)
    upload(pernosco_user, pernosco_group, pernosco_user_secret_key)

def upload_package_cmd():
    """
    Upload a package created using `pernosco-submit upload --dry-run' to Pernosco.
    """
    check_for_aws_command()
    check_title_and_url_args()
    check_privacy_policy_consent()

    pernosco_user = get_config_var('user')
    pernosco_group = get_config_var('group')
    pernosco_user_secret_key = get_config_var('user_secret_key')

    package_file_name = args.package

    upload_package(pernosco_user, pernosco_group, pernosco_user_secret_key, package_file_name)

def analyze_build_cmd():
    check_upload_analyze_requirements()

    (source_dirs, comp_dir_substitutions) = collect_source_dirs()

    repo_paths = package_source_files_extra(source_dirs, comp_dir_substitutions)
    package_gdbinit(repo_paths, "%s/extra_rr_trace_files/gdbinit"%args.output_dir)

if args.subcommand == 'upload':
    upload_cmd()
elif args.subcommand == 'upload-package':
    upload_package_cmd()
elif args.subcommand == 'keygen':
    keygen_cmd()
elif args.subcommand == 'analyze-build':
    analyze_build_cmd()
else:
    arg_parser.print_help(sys.stderr)
    sys.exit(1)
